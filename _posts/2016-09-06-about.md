---
layout: post
title: Subhojyoti Mukherjee
---

<!--
I am a Ph.D. candidate in the [Department of Electrical and Computer Engineering (ECE), University of Wisconsin-Madison](https://www.engr.wisc.edu/department/electrical-computer-engineering/). -->

I am a research scientist at Adobe Research. My expertise ranges from research and developing algorithms to training machine learning models, Reinforcement Learning, fine-tuning and alignment for LLMs. 

<!--
**Looking for full-time positions in the industry.**
-->

<!--[Download CV](/pdf/subho_cv.pdf) <br>-->
<a href="/pdf/subho_cv.pdf" target="_blank">Download CV</a><br>
Email: subhomuk [at] adobe [dot] com


<!-- # Statement and Vision
My broader vision is to build large-scale trustworthy Language, Vision and Machine Learning models. To achieve this I have looked into incorporating adaptive data collection strategies for LLM training and aligning LLMs with human feedback by collecting informative data. Building large-scale trustworthy Machine learning models is a challenging task, and to achieve this my past works have looked into various aspects of data collection for training models: 
1. Adaptive Data collection in Reinforcement Learning
2. Understanding Incontext learning for Decision transformers
3. Adaptive prompt design for LLMs, and aligning LLMs with human preferences through finetuning
4. Safety in Machine Learning -->


<h1>Work Experience</h1>
<table>
   <tr>
      <td class="edu">Adobe Research (San Jose)<br><em>Research Scientist/Engineer</em><br>(Mar 2025 - Present)</td>
      <td>
         <strong>Product:</strong><br>
         1) Pre-training and fine-tuning small LLMs for Adobe Document Cloud<br>
         2) Training LLMs and VLMs for agentic systems in Adobe Express
         <br><br>
         <strong>Research Areas:</strong> Understanding role of reasoning during pre-training of LLMs and scaling laws, alignment of LLMs during post-training, developing agentic systems for image editing and perturbation, fine-grained captioning with VLMs
         <br><br>
         <strong>Mentoring:</strong> Mentoring interns on projects spanning MLLMs for video understanding, LLMs for personalization, and learning to clarify with LLMs with LLM/VLM-as-a-judge.
      </td>
   </tr>
</table>

<h1>Education</h1>

<table>
   <!-- -->
   <!-- PhD Row -->
   <!-- -->
   <tr>
      <td class="edu">Ph.D. <br>(Fall 2019 to Feb 2025)<br></td>
      <td>
         at <a href="https://www.engr.wisc.edu/department/electrical-computer-engineering/" target="_blank">ECE, University of Wisconsin Madison</a><br>
         advised by 
         <a href="http://nowak.ece.wisc.edu/" target="_blank">Dr. Robert Nowak</a>, 
         <a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Dr. Josiah Hanna</a>, and
         <a href="https://qiaominxie.github.io/" target="_blank">Dr. Qiaomin Xie</a><br>
         <br>
         Areas of Research: Reinforcement Learning, Active Learning, incorporating deep active learning strategies for Large Language Models (LLMs), aligning Large Language Models with human feedback (RLHF), and understanding sequential decision-making using transformers (DT). 
         <br><br>
         PhD Thesis: Adaptive Data Collection for Policy Evaluation, Multi-task Learning and LLM Alignment
         <a href="pdf/Adaptive_Data_Collection_for_P.pdf" target="_blank">pdf</a>
         <br><br>
         (Joint) Masters Thesis: Active Sequential Hypothesis Testing with Extension to Active Regression and Multi-armed Bandits
         <a href="pdf/MS_Project_Report_UW_Madison.pdf" target="_blank">pdf</a><br>
      </td>
   </tr>
   <!-- -->
   <!-- Masters Row -->
   <!-- -->
   <tr>
      <td class="edu">M.S by Research<br>(2015 to 2018)</td>
      <td>
         at CSE, Indian Institute of Technology (IIT) Madras<br>
         advised by
         <a href="https://www.cse.iitm.ac.in/~ravi/" target="_blank">Dr. Balaraman Ravindran</a>, and
         <a href="https://doms.iitm.ac.in/index.php/nandan-s" target="_blank">Dr. Nandan Sudarsanam</a><br>
         <a href="http://rise.cse.iitm.ac.in/rise1/index.html" target="_blank">RISE Lab</a>
         <br><br>
         Areas of Research: Reinforcement learning, Stochastic and non-stochastic <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit" target="_blank">Multi-Armed Bandit</a> settings.<br><br>
         Masters Thesis: Finite-time Analysis of Frequentist Strategies for Multi-armed Bandits
         <a href="http://www.cse.iitm.ac.in/~ravi/papers/Subhojyoti_thesis.pdf" target="_blank">pdf</a>
      </td>
   </tr>
   <!-- -->
   <!-- Bachelors Row -->
   <!-- -->
   <tr>
      <td class="edu">Bachelor of Technology<br>(2009 to 2013)</td>
      <td>
         at Dept. of Computer Science and Engineering<br>
         <a href="http://www.msit.edu.in/" target="_blank">Meghnad Saha Institute of Technology, Kolkata</a><br>
         under <a href="http://www.wbut.ac.in/" target="_blank">West Bengal University of Technology, India</a>
      </td>
   </tr>
</table>


<h1>Research Internships</h1>

<table style="width:100%">
   <!-- -->
   <!-- Row 1 -->
   <!-- -->
   <tr>
      <td class="edu" style="width:35%">Amazon AWS AI, Santa Clara, USA<br>Summer 2024 (full-time)</td>
      <td>
         hosted by <a href="https://bkveton.com" target="_blank">Branislav Kveton</a>, <a href="https://sites.google.com/view/alalitha/">Anusha Lalitha </a> <br>
         and: Sailik Sengupta, Yifei Ma, Aniket Deshmukh, Gaurush Hiranandani. <br>
         <br>
         Area of Research: <b>Multi-objective alignment for LLMs.</b>
      </td>
   </tr>
   <!-- -->
   <!-- Row 2 -->
   <!-- -->
   <tr>
      <td class="edu">Amazon AWS AI, Santa Clara, USA<br>Fall 2023 (Part-time)</td>
      <td>
         hosted by <a href="https://bkveton.com" target="_blank">Branislav Kveton</a><br>
         and: Yifei Ma, Anusha Lalitha, Kousha Kalantiri, Ge Liu, Aniket Deshmukh, Anoop Deoras. <br>
         <br>
         Area of Research: <b>RLHF with LLMs.</b>
      </td>
   </tr>
   <!-- -->
   <!-- Row 3 -->
   <!-- -->
   <tr>
      <td class="edu">Amazon AWS AI, Santa Clara, USA<br>Summer 2023 (Full-time)</td>
      <td>
         hosted by <a href="https://bkveton.com" target="_blank">Branislav Kveton</a><br>
         and: Yifei Ma, Anusha Lalitha, Ge Liu, Aniket Deshmukh, Anoop Deoras. <br>
         <br>
         Area of Research: <b>Active In-Context Learning with LLMs.</b>
      </td>
   </tr>
   <!-- -->
   <!-- Row 4 -->
   <!-- -->
   <tr>
      <td class="edu">CMU, ECE Dept., Pittsburgh, USA<br>Summer 2019</td>
      <td>
         hosted by Prof. Gauri Joshi<br>
         Area of Research: <b>Structured Bandits.</b>
      </td>
   </tr>
   <!-- -->
   <!-- Row 5 -->
   <!-- -->
   <tr>
      <td class="edu">Adobe Research, San Jose, USA<br>Spring 2018</td>
      <td>
         hosted by <a href="https://bkveton.com" target="_blank">Branislav Kveton</a><br>
         Area of Research: <b>Item recommendation with Ranking and Bandits.</b>
      </td>
   </tr>
   <!-- -->
   <!-- Row 6 -->
   <!-- -->
   <tr>
      <td class="edu">INRIA, SequeL Lab, Lille, France<br>Fall 2017</td>
      <td>
         hosted by Odalric Maillard<br>
         Area of Research: <b>Non-stationary Bandits.</b>
      </td>
   </tr>
</table>



<!--
<h1>Research Focus and Selected works</h1>


## LLMs, RLHF, and Prompt Design

<table>
       <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_moo.png" alt="Multi-Objective Alignment of LLMs">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Multi-Objective Alignment of Large Language Models Through
Hypervolume Maximization</h2>
                <p>Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets. <a href="https://arxiv.org/pdf/2412.05469" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_opt_design_rlhf.png" alt="Optimal Design for RLHF">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Optimal Design for Human Feedback for Training Large Language Models (NeurIPS 2024 main conference)</h2>
                <p>We study the problem of data collection for learning preference
models. The key idea in our work is to generalize the optimal design, a method
for computing information gathering policies, to ranked lists. We
design efficient algorithms and experiment with several synthetic and real-world
datasets to show the statistical efficiency of our algorithms. <a href="https://arxiv.org/pdf/2404.13895" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_logged.png" alt="Performance of Logged Feedback in LLMs">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Off-Policy Evaluation from Logged Human Feedback using Large Language Models (ICML 2024 Workshop)</h2>
                <p>We study off-policy evaluation from logged human feedback. We formalize the problem, propose both model-based and model-free estimators for policy values, and show how to optimize them. We analyze unbiasedness of our estimators and evaluate them empirically wit Large Language Models. Our estimators can predict the absolute values of evaluated policies, rank them, and be optimized. <a href="https://arxiv.org/pdf/2406.10030" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_prompt.png" alt="Results in LLMs">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Optimal Design for Adaptive In-Context Prompt
Selection in Large Language Models </h2>
                <p>We use
active learning for adaptive prompt design and call it Active In-context Prompt
Design (AIPD). We design the LLM prompt by adaptively choosing few-shot informative 
examples using Optimal Design from a training set to optimize performance on a test set. We experiment in different tasks with small, medium, and large sized LLMs; and show that our proposed algorithms GO and SAL outperform other methods for
choosing few-shot examples in the LLM prompt at inference. <a href="https://arxiv.org/pdf/2404.08846" target="_blank">pdf</a>  </p>
            </td>
        </tr>
</table>

## Transformers, Multi-task Learning and Incontext Learning

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_pred.png" alt="PredeTor Performance in GPT2">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Pretraining Decision Transformers with Reward
Prediction for In-Context Multi-task Learning</h2>
                <p>We study multi-task RL problem where the goal is to
learn a near-optimal algorithm that minimizes cumulative regret. The tasks share
a common structure and the algorithm exploits the shared structure to minimize
the cumulative regret for an unseen but related test task. We use a transformer (CausalLM GPT2 model)
as a decision-making algorithm to learn this shared structure imlpicitly so as to generalize
to the test task. Our model outperforms other SOTA methods like DPT, and imitation learning algorithms like 
Algorithmic Distillation (AD) over a series of experiments on several structured
bandit problems. <a href="https://arxiv.org/pdf/2406.05064" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_mtl.png" alt="Representation Learning Performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Multi-task Representation Learning for Pure
Exploration in Bilinear Bandits (Neurips 2023)</h2>
                <p>We study multi-task representation learning for the problem of pure exploration
in bilinear bandits. We aim to find optimal items for multiple tasks that share a common low-dimensional
linear representation. We propose and analyze the algorithm
GOBLIN that uses an Optimal Design approach to optimize sample allocations
for learning the global representation as well as minimize the number of samples
needed to identify the optimal pair of items in individual tasks. <a href="https://arxiv.org/pdf/2311.00327" target="_blank">pdf</a> </p>
            </td>
        </tr>
</table>

## Reinforcement Learning 

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_speed.png" alt="Speed performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>SPEED: Experimental Design for Policy Evaluation in Linear
Heteroscedastic Bandits (AISTATS 2024)</h2>
                <p>In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are
given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such an optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. <a href="https://arxiv.org/pdf/2301.12357" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_revar.png" alt="ReVar Performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling (UAI 2022)</h2>
                <p>We study the problem of data collection for policy evaluation in Markov decision processes (MDPs). In policy evaluation, we are given a target
policy and asked to estimate the expected cumulative reward it will obtain in an environment formalized as an MDP. We develop and analyze the algorithm Reduced Variance Sampling (ReVar) algorithm that approximates the oracle strategy when the reward variances are unknown a priori and bound its sub-optimality compared to the oracle strategy. Finally, we empirically validate that ReVar leads to
policy evaluation with mean squared error comparable to the oracle strategy and significantly lower than simply running the target policy. <a href="https://arxiv.org/pdf/2203.04510" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_chernoff.png" alt="Active Learning Algorithm">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Chernoff Sampling for Active Testing and Extension to Active Regression (AISTATS 2022)</h2>
                <p>Active learning can reduce the number of samples needed to perform a hypothesis test and to estimate the parameters of a model. We revisit the work of Chernoff that described an asymptotically optimal algorithm for performing a hypothesis test. We obtain a novel sample complexity bound for Chernoff’s algorithm, with a non-asymptotic term that characterizes its performance at a fixed confidence level. We also develop an extension of Chernoff sampling that can be used to estimate the parameters of a wide variety of models and we obtain a non-asymptotic bound on the estimation error. We apply our extension of Chernoff sampling to actively learn neural network models and to estimate parameters in real-data linear and non-linear regression problems, where our approach performs favorably to state-of-the-art methods. <a href="https://arxiv.org/pdf/2012.08073" target="_blank">pdf</a> </p>
            </td>
        </tr>
</table>

## Safety in RL

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_saver.png" alt="SaVeR Performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular
MDP (ICML 2024)</h2>
                <p>We study safe data collection for the purpose of policy evaluation in tabular Markov
decision processes (MDPs). While prior work has considered behavior policy selection, in this paper,
we additionally consider a safety constraint on the behavior policy. We then introduce an algorithm SaVeR for this problem
that approximates the (best possible) safe oracle algorithm and bound the finite-sample mean squared error of
the algorithm while ensuring it satisfies the safety constraint. Finally, we show in simulations that
SaVeR produces low MSE policy evaluation while satisfying the safety constraint. <a href="https://arxiv.org/pdf/2406.02165" target="_blank">pdf</a> </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_safety_cd.png" alt="Safety Bandits">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits (UAI 2022)</h2>
                <p>We consider the setting of piecewise
i.i.d. bandits under a safety constraint. In this setting, there exists a finite number of
changepoints where the mean score of some or all actions (items) change simultaneously. We propose two actively adaptive algorithms for
this setting that satisfy the safety constraint, detect changepoints, and restart without the knowledge
of the number of changepoints or their locations. Empirically, we show that our safety-aware algorithms perform similarly
to the SOTA adaptive algorithms that do not satisfy the safety constraint. <a href="https://arxiv.org/pdf/2205.13689" target="_blank">pdf</a> </p>
            </td>
        </tr>
</table>
-->

# News

## 2025

- Our paper [From Selection to Generation: A Survey of LLM-based Active Learning](https://arxiv.org/pdf/2502.11767) is accepted in ACL 2025 (main conference).

- Our paper [Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit
Learning](https://arxiv.org/pdf/2406.05064) is accepted in RLC 2025 (main conference).
  
- Our paper Multi-task Representation Learning for Fixed Budget Pure-Exploration in Linear and Bilinear Bandits is accepted in RLC 2025 (main conference).

- Our paper [Logits are All We Need to Adapt Closed Models](https://arxiv.org/pdf/2502.06806) is accepted in ICML 2025 (main conference).

- I joined Adobe Research as a Research Scientist in the Natural Language Group.

- I succesfully defended my PhD dissertation at UW Madison.

- Our paper [From Selection to Generation: A Survey of LLM-based Active Learning](https://arxiv.org/pdf/2502.11767) is up in arxiv.

- Our paper [Logits are All We Need to Adapt Closed Models](https://arxiv.org/pdf/2502.06806) is up in arxiv.

- Our paper [Multi-Objective Alignment of Large Language Models Through
Hypervolume Maximization](https://arxiv.org/pdf/2412.05469) is up in arxiv.

## 2024

- Our paper [Optimal Design for Human Feedback](https://arxiv.org/pdf/2404.13895) was accepted at NeurIPS 2024 (main conference).

- Our paper [Optimal Design for Human Feedback](https://arxiv.org/pdf/2404.13895) was accepted at Models of Human Feedback for AI Alignment workshop in ICML 2024.

- Our paper [Off-Policy Evaluation from Logged Human Feedback](https://arxiv.org/pdf/2406.10030) was accepted at Models of Human Feedback for AI Alignment workshop in ICML 2024.

- Our paper [SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP](https://arxiv.org/pdf/2406.02165) was accepted at ICML 2024 (main conference).
  
- I will be returning as an Applied Scientist intern to Amazon AWS AI in the summer of 2024.

- Our paper [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits](https://arxiv.org/pdf/2301.12357.pdf) was accepted at AISTATS 2024.

- My internship at Amazon AWS AI has been extended (as part-time) till February 2024.

## 2023

- I won the Neural Information Processing Systems (Neurips) 2023 top reviewer award.

- I passed my prelim exam for the Doctoral degree.

- Our paper [Multi-task Representation Learning for Pure Exploration in Bilinear Bandits](https://arxiv.org/abs/2311.00327) was accepted at Neurips 2023.

- Our paper [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits](https://arxiv.org/pdf/2301.12357.pdf) was accepted at ICML 2023 Workshop The Many Facets of Preference-Based Learning.

- I won the top reviewer award at Uncertainty in Artificial Intelligence (UAI) 2023.

- I worked at the intersection of Active Learning and Large Language Models (LLMs) in my internship at Amazon AWS AI in the summer 2023. My internship has been extended (as part-time) till the end of Fall 2023.


## 2022

- Our paper [ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling](https://arxiv.org/abs/2203.04510) was accepted at Uncertainty in Artificial Intelligence (UAI) 2022.

- Our paper [Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits](https://arxiv.org/abs/2205.13689) was accepted at Uncertainty in Artificial Intelligence (UAI) 2022.

- I passed my Qualification Exam for the Doctoral degree.

- Our paper [Chernoff Sampling for Active Testing and Extension to Active Regression](https://arxiv.org/abs/2012.08073) was accepted at Artificial Intelligence and Statistics (AISTATS) 2022.

- Our paper [Nearly Optimal Algorithms for Level Set Estimation](https://arxiv.org/abs/2111.01768) was accepted at Artificial Intelligence and Statistics (AISTATS) 2022.

## 2021

- I got Master's Degree in Electrical Engineering from UW-Madison. Now moving on to finish my doctoral degree.

- Our paper [A Unified Approach to Translate Classical Bandit Algorithms to the Structured Bandit Setting](https://ieeexplore.ieee.org/abstract/document/9413628) was accepted in IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP 21).

## 2020

- Our paper [Generalized Chernoff Sampling: A New Perspective on Structured Bandit Algorithms”,](https://arxiv.org/abs/1810.08164) was accepted at [Theoretical Foundations Of Reinforcement Learning ICML 2020 Workshop](https://wensun.github.io/rl_theory_workshop_2020_ICML.github.io/).

- Our paper [A Unified Approach to Translate Classical Bandit Algorithms to the Structured Bandit Setting](https://ieeexplore.ieee.org/abstract/document/9276444) was accepted in IEEE Journal on Selected Areas in
Information Theory (2020).

## 2019

- Our paper [Distribution-dependent and Time-uniform Bounds for Piecewise i.i.d Bandits](https://arxiv.org/abs/1905.13159) was accepted at [Reinforcement Learning for Real Life ICML 2019 Workshop](https://sites.google.com/view/RL4RealLife).

- Going to spend the summer of 2019 as a Research Associate in the Department of Electrical and Computer Engineering (ECE) at Carnegie Mellon University (CMU) working with Professor [Gauri Joshi](https://www.andrew.cmu.edu/user/gaurij/) and [Osman Yagan](http://www.andrew.cmu.edu/user/oyagan/).

- I received the 2019 Chancellor's Opportunity Fellowship award at the University of Wisconsin-Madison.


<!--
# Resume

You can find my full resume here ([Resume](/pdf/subho_cv.pdf)).

# Contact 

smukherjee27 [at] wisc [dot] edu
-->
