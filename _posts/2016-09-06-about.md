---
layout: post
title: Subhojyoti Mukherjee
---
I am a Ph.D. candidate in the [Department of Electrical and Computer Engineering (ECE), University of Wisconsin-Madison](https://www.engr.wisc.edu/department/electrical-computer-engineering/) (from Fall 2019 to Fall 2024 expected). I am advised by [Dr. Robert Nowak](http://nowak.ece.wisc.edu/), [Dr. Josiah Hanna](https://pages.cs.wisc.edu/~jphanna/), and [Dr. Qiaomin Xie](https://qiaominxie.github.io/). I am a graduate research assistant at [Wisconsin Institute of Discovery](https://wid.wisc.edu/people/subhojyoti-mukherjee/). I work in the area of Reinforcement Learning, Active Learning, incorporating deep active learning strategies for Large Language Models (LLMs), aligning Large Language Models with human feedback (RLHF), and understanding sequential decision-making using transformers (DT). 

My broader vision is to build large-scale trustworthy Language, Vision and Machine Learning models. To achieve this I have looked into incorporating adaptive data collection strategies for LLM training and aligning LLMs with human feedback by collecting informative data. Building large-scale trustworthy Machine learning models is a challenging task, and to achieve this my past works have looked into various aspects of data collection for training models: 1) Adaptive Data collection in Reinforcement Learning, 2) Understanding Incontext learning for Decision transformers 3) Adaptive prompt design for LLMs for few-shot learning, and Data collection from Human feedback for Aligning LLMs with human preferences 4) Safety in Machine Learning.

My expertise ranges from research and developing algorithms to training machine learning models, Reinforcement Learning, fine-tuning LLMs, and prompt designing for LLMs. This expertise is crucial to build large-scale real-life systems for user interaction and understanding user preference from data. 

   I was an M.S (Research) scholar in the [Computer Science and Engineering Department, IIT Madras](https://www.cse.iitm.ac.in/) from January 2015 to July 2018. I was advised by [Dr. Balaraman Ravindran](https://www.cse.iitm.ac.in/~ravi/) (CSE Department, IIT Madras) and [Dr. Nandan Sudarsanam](https://doms.iitm.ac.in/index.php/nandan-s) (Department of Management Studies, IIT Madras). I was associated with the [RISE](http://rise.cse.iitm.ac.in/rise1/index.html) lab at IIT Madras. I worked in the area of RL, stochastic and non-stochastic [Multi-Armed Bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) settings.
   
   I completed my B.Tech from [Meghnad Saha Institute of Technology](http://www.msit.edu.in/), Kolkata under [West Bengal University of Technology](http://www.wbut.ac.in/) in 2013.

# Research Focus and Selected works

## LLMs, RLHF, and Prompt Design

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_prompt.png" alt="Results in LLMs">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Optimal Design for Adaptive In-Context Prompt
Tuning in Large Language Models</h2>
                <p>One emergent ability of large language models (LLMs) is that query-specific
examples can be included in the prompt at inference time. In this work, we use
active learning for adaptive prompt design and call it Active In-context Prompt
Design (AIPD). We design the LLM prompt by adaptively choosing few-shot
examples from a training set to optimize performance on a test set. The training
examples are initially unlabeled and we obtain the label of the most informative
ones, which maximally reduces uncertainty in the LLM prediction. We propose two
algorithms, GO and SAL, which differ in how the few-shot examples are chosen.
We analyze these algorithms in linear models: first GO and then use its equivalence
with SAL. We experiment with many different tasks in small, medium-sized, and
large language models; and show that GO and SAL outperform other methods for
choosing few-shot examples in the LLM prompt at inference time.</p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_opt_design_rlhf.png" alt="Optimal Design for RLHF">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Optimal Design for Human Feedback for Training Large Language Models</h2>
                <p>Learning of preference models from human feedback has been central to recent
advances in artificial intelligence. Motivated by the cost of obtaining high-quality
human annotations, we study the problem of data collection for learning preference
models. The key idea in our work is to generalize the optimal design, a method
for computing information gathering policies, to ranked lists. To show the generality of our ideas, we study both absolute and relative feedback on the lists. We
design efficient algorithms for both settings and analyze them. We prove that our
preference model estimators improve with more data and so does the ranking error
under the estimators. Finally, we experiment with several synthetic and real-world
datasets to show the statistical efficiency of our algorithms.</p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_logged.png" alt="Performance of Logged Feedback in LLMs">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Off-Policy Evaluation from Logged Human Feedback using Large Language Models</h2>
                <p>Learning of preference models from human feedback has been central to recent
advances in artificial intelligence. Motivated by the cost of obtaining high-quality
human annotations, we study the problem of data collection for learning preference
models. The key idea in our work is to generalize the optimal design, a method
for computing information gathering policies, to ranked lists. To show the generality of our ideas, we study both absolute and relative feedback on the lists. We
design efficient algorithms for both settings and analyze them. We prove that our
preference model estimators improve with more data and so does the ranking error
under the estimators. Finally, we experiment with several synthetic and real-world
datasets to show the statistical efficiency of our algorithms.</p>
            </td>
        </tr>
</table>

## Transformers, Multi-task Learning and Incontext Learning

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_pred.png" alt="PredeTor Performance in GPT2">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Pretraining Decision Transformers with Reward
Prediction for In-Context Multi-task Learning</h2>
                <p>In this paper, we study multi-task RL problem where the goal is to
learn a near-optimal algorithm that minimizes cumulative regret. The tasks share
a common structure and the algorithm exploits the shared structure to minimize
the cumulative regret for an unseen but related test task. We use a transformer (CausalLM GPT2 model)
as a decision-making algorithm to learn this shared structure so as to generalize
to the test task. Once trained the transformer weights are frozen. During inference time, it selects action using the reward
predictions employing various exploration strategies in-context for an unseen test
task. We show that our model outperforms other SOTA methods like DPT, and imitation learning algorithms like 
Algorithmic Distillation (AD) over a series of experiments on several structured
bandit problems (linear, bilinear, latent, non-linear). Interestingly, we show that
our algorithm, without the knowledge of the underlying problem structure, can
learn a near-optimal policy in-context by leveraging the shared structure across
diverse tasks. </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_mtl.png" alt="Representation Learning Performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Multi-task Representation Learning for Pure
Exploration in Bilinear Bandits</h2>
                <p>We study multi-task representation learning for the problem of pure exploration
in bilinear bandits. In bilinear bandits, an action takes the form of a pair of items
from two different entity types and the reward is a bilinear function of the known
feature vectors of the items. In the multi-task bilinear bandit problem, we aim
to find optimal items for multiple tasks that share a common low-dimensional
linear representation. The objective is to leverage this characteristic to expedite the
process of identifying the best pair of items for all tasks. We propose the algorithm
GOBLIN that uses an experimental design approach to optimize sample allocations
for learning the global representation as well as minimize the number of samples
needed to identify the optimal pair of items in individual tasks. </p>
            </td>
        </tr>
</table>

## Reinforcement Learning 

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_speed.png" alt="Speed performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>SPEED: Experimental Design for Policy Evaluation in Linear
Heteroscedastic Bandits</h2>
                <p>In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are
given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such an optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_revar.png" alt="ReVar Performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling</h2>
                <p>We study the problem of data collection for policy evaluation in Markov decision processes (MDPs). In policy evaluation, we are given a target
policy and asked to estimate the expected cumulative reward it will obtain in an environment formalized as an MDP. We develop theory for optimal
data collection within the class of tree-structured MDPs by first deriving an oracle data collection strategy that uses knowledge of the variance of
the reward distributions. We then introduce the Reduced Variance Sampling (ReVar) algorithm that approximates the oracle strategy when the reward variances are unknown a priori and bound its sub-optimality compared to the oracle strategy. Finally, we empirically validate that ReVar leads to
policy evaluation with mean squared error comparable to the oracle strategy and significantly lower than simply running the target policy. </p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_chernoff.png" alt="Active Learning Algorithm">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Chernoff Sampling for Active Testing and Extension to Active Regression</h2>
                <p>Active learning can reduce the number of samples needed to perform a hypothesis test and to estimate the parameters of a model. In
this paper, we revisit the work of Chernoff that described an asymptotically optimal algorithm for performing a hypothesis test. We obtain a novel sample complexity bound for Chernoff’s algorithm, with a non-asymptotic term that characterizes its performance at a fixed confidence level. We also develop an extension of Chernoff sampling that can be used to estimate the parameters of a wide variety of models and we obtain a non-asymptotic bound on the estimation error. We apply our extension of Chernoff sampling to actively learn neural network models and to estimate parameters in real-data linear and non-linear regression problems, where our approach performs favorably to state-of-the-art methods.</p>
            </td>
        </tr>
</table>

## Safety in RL

<table>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_saver.png" alt="SaVeR Performance">
            </td>
            <td style="border: none; width: 50%;">
                <h2>SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular
MDP</h2>
                <p>In this paper, we study safe data collection for the purpose of policy evaluation in tabular Markov
decision processes (MDPs). In policy evaluation, we are given a target policy and asked to estimate
the expected cumulative reward it will obtain. Policy evaluation requires data and we are interested
in the question of what behavior policy should collect the data for the most accurate evaluation
of the target policy. While prior work has considered behavior policy selection, in this paper,
we additionally consider a safety constraint on the behavior policy. We then introduce an algorithm SaVeR for this problem
that approximates the (best possible) safe oracle algorithm and bound the finite-sample mean squared error of
the algorithm while ensuring it satisfies the safety constraint. Finally, we show in simulations that
SaVeR produces low MSE policy evaluation while satisfying the safety constraint.</p>
            </td>
        </tr>
        <tr>
            <td style="border: none; width: 50%;">
                <img src="/img/origin_image_safety_cd.png" alt="Safety Bandits">
            </td>
            <td style="border: none; width: 50%;">
                <h2>Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits</h2>
                <p>In this paper, we consider the setting of piecewise
i.i.d. bandits under a safety constraint. In this piecewise i.i.d. setting, there exists a finite number of
changepoints where the mean of some or all actions (items) change simultaneously. We propose two actively adaptive algorithms for
this setting that satisfy the safety constraint, detect changepoints, and restart without the knowledge
of the number of changepoints or their locations. Empirically, we show that our safety-aware algorithms perform similarly
to the state-of-the-art actively adaptive algorithms that do not satisfy the safety constraint.</p>
            </td>
        </tr>
</table>

# News

## 2024

- Our paper [Optimal Design for Human Feedback](https://arxiv.org/pdf/2404.13895) was accepted at Models of Human Feedback for AI Alignment workshop in ICML 2024.

- Our paper [Off-Policy Evaluation from Logged Human Feedback](https://arxiv.org/pdf/2406.10030) was accepted at Models of Human Feedback for AI Alignment workshop in ICML 2024.

- Our paper [SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP](https://arxiv.org/pdf/2406.02165) was accepted at ICML 2024 (main conference).
  
- I will be returning as an Applied Scientist intern to Amazon AWS AI in the summer of 2024.

- Our paper [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits](https://arxiv.org/pdf/2301.12357.pdf) was accepted at AISTATS 2024.

- My internship at Amazon AWS AI has been extended (as part-time) till February 2024.

## 2023

- I won the Neural Information Processing Systems (Neurips) 2023 top reviewer award.

- I passed my prelim exam for the Doctoral degree.

- Our paper [Multi-task Representation Learning for Pure Exploration in Bilinear Bandits](https://arxiv.org/abs/2311.00327) was accepted at Neurips 2023.

- Our paper [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits](https://arxiv.org/pdf/2301.12357.pdf) was accepted at ICML 2023 Workshop The Many Facets of Preference-Based Learning.

- I won the top reviewer award at Uncertainty in Artificial Intelligence (UAI) 2023.

- I worked at the intersection of Active Learning and Large Language Models (LLMs) in my internship at Amazon AWS AI in the summer 2023. My internship has been extended (as part-time) till the end of Fall 2023.


## 2022

- Our paper [ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling](https://arxiv.org/abs/2203.04510) was accepted at Uncertainty in Artificial Intelligence (UAI) 2022.

- Our paper [Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits](https://arxiv.org/abs/2205.13689) was accepted at Uncertainty in Artificial Intelligence (UAI) 2022.

- I passed my Qualification Exam for the Doctoral degree.

- Our paper [Chernoff Sampling for Active Testing and Extension to Active Regression](https://arxiv.org/abs/2012.08073) was accepted at Artificial Intelligence and Statistics (AISTATS) 2022.

- Our paper [Nearly Optimal Algorithms for Level Set Estimation](https://arxiv.org/abs/2111.01768) was accepted at Artificial Intelligence and Statistics (AISTATS) 2022.

## 2021

- I got Master's Degree in Electrical Engineering from UW-Madison. Now moving on to finish my doctoral degree.

- Our paper [A Unified Approach to Translate Classical Bandit Algorithms to the Structured Bandit Setting](https://ieeexplore.ieee.org/abstract/document/9413628) was accepted in IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP 21).

## 2020

- Our paper [Generalized Chernoff Sampling: A New Perspective on Structured Bandit Algorithms”,](https://arxiv.org/abs/1810.08164) was accepted at [Theoretical Foundations Of Reinforcement Learning ICML 2020 Workshop](https://wensun.github.io/rl_theory_workshop_2020_ICML.github.io/).

- Our paper [A Unified Approach to Translate Classical Bandit Algorithms to the Structured Bandit Setting](https://ieeexplore.ieee.org/abstract/document/9276444) was accepted in IEEE Journal on Selected Areas in
Information Theory (2020).

## 2019

- Our paper [Distribution-dependent and Time-uniform Bounds for Piecewise i.i.d Bandits](https://arxiv.org/abs/1905.13159) was accepted at [Reinforcement Learning for Real Life ICML 2019 Workshop](https://sites.google.com/view/RL4RealLife).

- Going to spend the summer of 2019 as a Research Associate in the Department of Electrical and Computer Engineering (ECE) at Carnegie Mellon University (CMU) working with Professor [Gauri Joshi](https://www.andrew.cmu.edu/user/gaurij/) and [Osman Yagan](http://www.andrew.cmu.edu/user/oyagan/).

- I received the 2019 Chancellor's Opportunity Fellowship award at the University of Wisconsin-Madison.


# Resume

You can find my full resume here ([Resume](/pdf/subho_cv.pdf)).

# Contact 

smukherjee27 [at] wisc [dot] edu
